---
title: "6.5 Overfitting, Regularization, and IC"
output: pdf
---

Informative priors reduce overfitting by reducing the sensitivity of a model to a sample. Some
of the information in a sample is irregular, not a recurring feature of the process of interest.

## Model Comparison
Remove NAs, rescale one of the explanatory vars.
```{r}
library(rethinking)
library(tidyverse)
data(milk)
d <- milk
d <- d %>% filter(complete.cases(d)) %>% 
  mutate(neocortex = neocortex.perc/100)
```

Four models with flats priors (i.e. no regularization) and different number of parameters.
Note: using log and then exp for sigma is a way to constraint the std deviation of the outcome to be positive.
```{r}
a.start <- mean(d$kcal.per.g)
sigma.start <- log(sd(d$kcal.per.g))
m1 <- quap(
  alist(
    kcal.per.g~dnorm(a, exp(log.sigma))
  ),
  data = d, start = list(a = a.start, log.sigma = sigma.start)
)

m2 <- quap(
  alist(
    kcal.per.g~dnorm(mu, exp(log.sigma)),
    mu <-  a+bn*neocortex
  ),
  data = d, start = list(a = a.start, bn = 0, log.sigma = sigma.start)
)

m3 <- quap(
  alist(
    kcal.per.g~dnorm(mu, exp(log.sigma)),
    mu <-  a+bm*log(mass)
  ),
  data = d, start = list(a = a.start, bm = 0, log.sigma = sigma.start)
)

m4 <- quap(
  alist(
    kcal.per.g~dnorm(mu, exp(log.sigma)),
    mu <-  a+bn*neocortex+bm*log(mass)
  ),
  data = d, start = list(a = a.start, bn = 0, bm = 0, log.sigma = sigma.start)
)
```

Now, with 4 sets of estimates and 4 deviances, in hand, we'll look at: comparing the models on the basis of WAIC values and on the basis of parameter estimates.

Goal is to see how predictions and estimates change as predictors are added and subtracted from the model.

### Comparing WAIC values
```{r}
WAIC(m4)
milk.models <- compare(m1, m2, m3, m4)
milk.models 
```
WAIC & SE: the WAIC estimate and its std error. smaller WAIC, better estimand out-of-sample deviance.

dWAIC: difference between each WAIC and the lowest WAIC. note that since only relative deviance matters, this column shows differences in that relative fashion.

dSE: std. error of difference between each WAIC and the top-ranked model (lowest WAIC).

pWAIC: estimated effective number of parameters. provides a clue about how flexible each model is in fitting the sample.

weight: Akaike weight, they are transformed information criterion values. an estimate of the probability that the model will make the best predictions on new data, conditional on the set of models being considered.

```{r}
plot(milk.models, SE = TRUE, dSE = TRUE)
```
In the plot above, the filled points are in-sample deviance, the open points are WAIC. The std error of each WAIC is the dark line segment. The dWAIC & dSE are the triangles and the grey line.

### Comparing estimates
In addition to comparing models on the basis of expected test deviance, it is useful to compare them on the basis of parameter estimates.
```{r}
library(rethinking)
coeftab(m1, m2, m3 , m4)
```
nobs are number of ovbservations used to fit each model.
```{r}
plot(coeftab(m1, m2, m3 , m4))
```
Each point in the plot is a MAP estimate and each line segment is a 89% HPDI. The vertical dashed line is at 0.


The question this adresses is:
How do parameter estimates change as predictors are added and subtracted from the model? Does it parameter's posterior distribution remain stable when other predictors are added or removed?

## Model Averaging
As a review, let's simulate & plot counterfactual predictions (posterior predictive distribution) for the minimum-WAIC model (m4).
```{r}
#counterfactual prediction computation
#neo cortex seq
ncseq <- seq(from = 0.5, to = 0.8, length.out = 30)
d.predict <- list(kcal.per.g = rep(0, 30), #empty outcome
                  neocortex = ncseq,
                  mass = rep(4.5, 30)
                  )

pred.m4 <- link(m4, data = d.predict)
mu = apply(pred.m4, 2, mean)
mu.PI = apply(pred.m4, 2, PI)

plot(kcal.per.g ~ neocortex, data = d,
     col = rangi2, pch = 16)
lines(ncseq, mu, lty = 2)
lines(ncseq, mu.PI[1,], lty = 2)
lines(ncseq, mu.PI[2,], lty = 2)
```
Now, let's compute and add model averaged posterior predictions: computing an ENSEMBLE of the posterior predictions from each model, weighted by the model's WAIC weight.

1. compute WAIC for each model.
2. compute weight for each model.
3. compute linear model & simulated outcomes for each model.
4. combine these values into an *ensemble of predictions*, using the model weights as proportions.

```{r}
milk.ensemble <- ensemble(m1, m2, m3, m4, data = d.predict)
mu <-  apply(milk.ensemble$link, 2, mean)
mu.PI <- apply(milk.ensemble$link, 2, PI)
plot(kcal.per.g ~ neocortex, data = d,
     col = rangi2, pch = 16)
lines(ncseq, mu)
shade(mu.PI, ncseq)
```


